{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_tuner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_SIZE = (96, 96)\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.4\n",
    "VAL_SIZE = 0.2\n",
    "DATA_DIR = \"data\"\n",
    "ID_DRIVE = {\n",
    "    \"empty\": \"1NH3MTPqwPk25Zz127D6SP45dNnFkQB5e&confirm=t\",\n",
    "    \"crossedout\": \"19_8vPdpYqO1WrM5gbchrRNDEXxjiIKLC&confirm=t\",\n",
    "    \"confirmed\": \"1iQ8x12DvT7s15IUDegI87aJtcUMECXqU&confirm=t\",\n",
    "}\n",
    "EPOCHS = 50\n",
    "MAX_TRIALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
    "el_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, start_from_epoch=5, verbose=1, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "from functools import partial\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def obtain_data(directory=\"data\"):\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"O diretório {directory} já existe, pulando o download dos dados.\")\n",
    "        return\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        print(\"created temporary directory\", tmpdirname)\n",
    "        for name, id in ID_DRIVE.items():\n",
    "            gdown.download(id=id, output=os.path.join(tmpdirname, f\"{name}.zip\"))\n",
    "\n",
    "        for file in os.listdir(tmpdirname):\n",
    "            with ZipFile(os.path.join(tmpdirname, file), \"r\") as zip:\n",
    "                zip.extractall(path=directory)\n",
    "\n",
    "\n",
    "def get_label(file_path, class_names):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    one_hot = parts[-2] == class_names\n",
    "    return tf.argmax(one_hot)\n",
    "\n",
    "\n",
    "def decode_img(img, img_size):\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return tf.image.resize(img, img_size)\n",
    "\n",
    "\n",
    "def process_path(file_path, class_names, img_size):\n",
    "    label = get_label(file_path, class_names)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img, img_size)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def configure_for_performance(ds, batch_size):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def train_test_val_img_dataset(\n",
    "    directory=\"data\", seed=1337, test_size=0.2, val_size=0.2, batch_size=32, image_size=(224, 224)\n",
    "):\n",
    "    data_dir = pathlib.Path(directory).with_suffix(\"\")\n",
    "\n",
    "    list_ds = tf.data.Dataset.list_files(str(data_dir / \"*/*\"), shuffle=False)\n",
    "    image_count = list_ds.cardinality().numpy()\n",
    "    list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False, seed=seed)\n",
    "\n",
    "    class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
    "\n",
    "    train_ds = list_ds.skip(test_size * image_count + val_size * image_count)\n",
    "    val_ds = list_ds.take(val_size * image_count)\n",
    "    test_ds = list_ds.skip(val_size * image_count).take(test_size * image_count)\n",
    "\n",
    "    process_path_p = partial(process_path, class_names=class_names, img_size=image_size)\n",
    "\n",
    "    train_ds = train_ds.map(process_path_p, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(process_path_p, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(process_path_p, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = configure_for_performance(train_ds, batch_size)\n",
    "    val_ds = configure_for_performance(val_ds, batch_size)\n",
    "    test_ds = configure_for_performance(test_ds, batch_size)\n",
    "\n",
    "    return train_ds, test_ds, val_ds, class_names\n",
    "\n",
    "\n",
    "def count_labels(ds):\n",
    "    label_counts = {}\n",
    "    for _, labels in ds:\n",
    "        labels_numpy = labels.numpy()\n",
    "\n",
    "        for label_value in labels_numpy:\n",
    "            label_value = int(label_value)\n",
    "            if label_value in label_counts:\n",
    "                label_counts[label_value] += 1\n",
    "            else:\n",
    "                label_counts[label_value] = 1\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "def generate_class_weigths(class_counts):\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {}\n",
    "    # Pesos para cada classe (inversamente proporcionais às frequências)\n",
    "    for class_name, count in class_counts.items():\n",
    "        class_weights[class_name] = total_samples / (len(class_counts) * count)\n",
    "\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga dos dados\n",
    "\n",
    "Os dados usados para a construção do modelo são do conjunto [MC Answer Boxes Dataset](https://sites.google.com/view/mcq-dataset), que contém diversos testes de múltipla escolha respondidos. Neste conjunto de dados, já temos as caixas de resposta de cada teste extraída e com a marcação entre \"confirmada\", \"vazia\" ou \"cancelada\".\n",
    "\n",
    "Os dados estão disponíveis publicamente no Google Drive, e é possível fazer o download automático usando o pacote `gdown`.\n",
    "\n",
    "Um diretório temporário é criado, os arquivos são baixados do Google Drive para este diretório e então são descompactados criando a pasta data.\n",
    "\n",
    "Dentro da pasta data, são criados diretórios para cada classe (\"confirmed\", \"empty\" e \"crossedout\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtain_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do dataset\n",
    "\n",
    "Os dados serão divididos em três partes:\n",
    "\n",
    "1. **Treino** : Dados que serão utilizados para treinar o modelo.\n",
    "2. **Validação**: Dados que serão utilizados para avaliar o modelo durante o treinamento.\n",
    "3. **Teste**: Dados que serão utilizados para avaliar o modelo após o treinamento.\n",
    "\n",
    "A separação em validação e teste garante que a avaliação do modelo será feita com dados que não foram vistos em nenhum momento durante o treinamento - nem mesmo na validação.\n",
    "\n",
    "O conjunto de teste é salvo para reutilização na obtenção de métricas e visualização de previsões.\n",
    "Além disso, uma pequena quantidade de batches é salva para teste automatizado na API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, val_ds, class_names = train_test_val_img_dataset(\n",
    "    DATA_DIR, test_size=TEST_SIZE, val_size=VAL_SIZE, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=1337\n",
    ")\n",
    "# test_ds.save(\"datasets/ds_test\")\n",
    "# test_ds.shuffle(test_ds.cardinality()).take(50).save(\"datasets/ds_test_sample\")\n",
    "del test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados são referentes a imagens das caixas de resposta extraídas de testes de múltipla escolha. Cada imagem é de uma caixa de resposta, e a marcação da caixa é feita como \"confirmed\" (confirmada), \"empty\" (vazia) ou \"crossedout\" (cancelada).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "    label = label_batch[i]\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geralmente, quando respondendo um teste de múltipla escolha, não é muito frequente as vezes em que cancelamos uma resposta. Portanto, é esperado que a classe \"crossedout\" possua uma frequência menor que as outras classes.\n",
    "\n",
    "Além disso, testes múltiplas escolhas usualmente possuem mais do que duas alteranativas, com apenas uma delas sendo a alternativa correta. Portanto, também é esperado que a classe \"empty\" possua uma frequência menor que a classe \"confirmed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_freq = count_labels(train_ds)\n",
    "\n",
    "{class_names[k]: v for k, v in train_class_freq.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para corrigir esse desbalanceamento, uma possível alternativa é aumentar o peso dessa classe durante o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = generate_class_weigths(train_class_freq)\n",
    "{class_names[k]: v for k, v in class_weights.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo\n",
    "\n",
    "O modelo utilizado é um modelo treinado usando trasnfer learning com o modelo Xception.\n",
    "\n",
    "Foi incluído um passo de preprocessamento para ajustar o tamanho das imagens para o tamanho desejado pelo modelo.\n",
    "Por mais que os conjuntos de dados usados no treino e no teste já estejam com o tamanho correto, no momento da previsão podem ser passadas imagens de qualquer tamanho que serão ajustadas para o tamanho correto.\n",
    "\n",
    "Além disso, foi incluído um passo de normalização, que ajusta os valores para o intervalo [-1, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes, dropout, learning_rate):\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape + (3,),\n",
    "        include_top=False,\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = keras.Input(shape=[None, None, 3])\n",
    "    # Step de preprocessamento para ajudar na previsão - isso me permite\n",
    "    # usar imagens de qualquer tamanho na hora de realizar a previsão.\n",
    "    naive_resize = keras.layers.Resizing(*input_shape)(inputs)\n",
    "\n",
    "    scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "    x = scale_layer(naive_resize)\n",
    "    x = base_model(x, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    if dropout:\n",
    "        x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização dos hiperparâmetros\n",
    "\n",
    "Foram escolhidos para otimização os seguintes hiperparâmetros:\n",
    "\n",
    "1. Learning rate: Taxa de aprendizado do modelo.\n",
    "2. Dropout: Utilização ou não de dropout.\n",
    "\n",
    "A otimização é realizada utilizando uma busca aleatória no espaço de hiperparâmetros, com no máximo 5 tentativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_shape = IMAGE_SIZE\n",
    "    num_classes = len(class_names)\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    model = make_model(input_shape, num_classes, dropout, learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_acc\",\n",
    "    max_trials=MAX_TRIALS,\n",
    "    project_name=\"otimizacao_hiperparametros\",\n",
    "    seed=1337,\n",
    "    # overwrite=True,\n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[tb_callback, el_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumo da busca pelos melhores hiperparâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste do modelo\n",
    "\n",
    "Após obter a melhor combinação de hiperparâmetros, o modelo pode ser treinado novamente, com a melhor combinação dos hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)\n",
    "model = build_model(best_hps[0])\n",
    "model.summary(show_trainable=True)\n",
    "del tuner, best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo é salvo a cada época através do callback ModelCheckpoint. Se a *validation loss* parar de diminuir, o treinamento é interrompido de forma antecipada através do callback EarlyStopping. Além disso, são salvos logs para visualização do treinamento no TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "callbacks = [\n",
    "    # keras.callbacks.ModelCheckpoint(\"models/checkpoint/save_at_{epoch}.keras\"),\n",
    "    el_callback,\n",
    "    tb_callback,\n",
    "]\n",
    "\n",
    "model.fit(train_ds, epochs=EPOCHS, callbacks=callbacks, validation_data=val_ds, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo treinado é salvo para uso posterior pela API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/omr_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
